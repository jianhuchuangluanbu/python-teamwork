{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "第三部分",
   "id": "f20e9ab538ff2db7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NLTK中的nltk.tokenize模块\n",
    "\n",
    "nltk.tokenize模块专门用于分词，即将文本拆分成单词、句子或其他标记的过程。"
   ],
   "id": "5e10f66825a3ef84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. word_tokenize\n",
    "\n",
    "word_tokenize函数可以将字符串分割成单词列表，分词的目的是将一段连续的文本分解为更小的单位，以便于后续的分析和处理。\n",
    "\n",
    "使用前需要先下载punkt模型\n",
    "\n",
    "Punkt模型是一个基于无监督学习的句子边界检测工具，专门用于句子分割"
   ],
   "id": "667718ad2fdb9f0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b9f8b320a021b9a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:48:44.162746Z",
     "start_time": "2024-06-16T06:48:43.529665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ],
   "id": "ba01513c9f0efbc9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Honjoutx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "3209c96be61d67e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:59:39.714511Z",
     "start_time": "2024-06-16T06:59:39.711428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = [\n",
    "    {\n",
    "        \"text\": \"The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (DPRK) and find ways to revive dialogue with the Government, the UN Security Council heard on Wednesday. \"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"China and Russia opposed the meeting and called for a procedural vote by the 15 members, which was defeated. \"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"One consequence is that divided families are even more divided. No departures means no reunification with families abroad.\"\n",
    "    }\n",
    "]"
   ],
   "id": "7248357383c08f26",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:12:49.057257Z",
     "start_time": "2024-06-14T11:12:49.049543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = word_tokenize(text)\n",
    "    print(result)"
   ],
   "id": "ab52b36a1b1d7cb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'international', 'community', 'must', 'continue', 'to', 'pay', 'close', 'attention', 'to', 'the', 'troubling', 'human', 'rights', 'situation', 'in', 'the', 'Democratic', 'People', '’', 's', 'Republic', 'of', 'Korea', '(', 'DPRK', ')', 'and', 'find', 'ways', 'to', 'revive', 'dialogue', 'with', 'the', 'Government', ',', 'the', 'UN', 'Security', 'Council', 'heard', 'on', 'Wednesday', '.']\n",
      "['China', 'and', 'Russia', 'opposed', 'the', 'meeting', 'and', 'called', 'for', 'a', 'procedural', 'vote', 'by', 'the', '15', 'members', ',', 'which', 'was', 'defeated', '.']\n",
      "['One', 'consequence', 'is', 'that', 'divided', 'families', 'are', 'even', 'more', 'divided', '.', 'No', 'departures', 'means', 'no', 'reunification', 'with', 'families', 'abroad', '.']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:49:22.956773Z",
     "start_time": "2024-06-16T06:49:22.953504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = word_tokenize(text)\n",
    "    word_freq = Counter(result)\n",
    "    print(word_freq)\n"
   ],
   "id": "89843febb99016c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 4, 'to': 3, 'The': 1, 'international': 1, 'community': 1, 'must': 1, 'continue': 1, 'pay': 1, 'close': 1, 'attention': 1, 'troubling': 1, 'human': 1, 'rights': 1, 'situation': 1, 'in': 1, 'Democratic': 1, 'People': 1, '’': 1, 's': 1, 'Republic': 1, 'of': 1, 'Korea': 1, '(': 1, 'DPRK': 1, ')': 1, 'and': 1, 'find': 1, 'ways': 1, 'revive': 1, 'dialogue': 1, 'with': 1, 'Government': 1, ',': 1, 'UN': 1, 'Security': 1, 'Council': 1, 'heard': 1, 'on': 1, 'Wednesday': 1, '.': 1})\n",
      "Counter({'and': 2, 'the': 2, 'China': 1, 'Russia': 1, 'opposed': 1, 'meeting': 1, 'called': 1, 'for': 1, 'a': 1, 'procedural': 1, 'vote': 1, 'by': 1, '15': 1, 'members': 1, ',': 1, 'which': 1, 'was': 1, 'defeated': 1, '.': 1})\n",
      "Counter({'divided': 2, 'families': 2, '.': 2, 'One': 1, 'consequence': 1, 'is': 1, 'that': 1, 'are': 1, 'even': 1, 'more': 1, 'No': 1, 'departures': 1, 'means': 1, 'no': 1, 'reunification': 1, 'with': 1, 'abroad': 1})\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:59:29.050626Z",
     "start_time": "2024-06-16T06:59:28.566708Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('stopwords')",
   "id": "190596286a8b0737",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Honjoutx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:01:19.799134Z",
     "start_time": "2024-06-16T07:01:19.790340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = word_tokenize(text) \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [word for word in result if word.lower() not in stop_words and word.isalnum()]\n",
    "    freq = Counter(filtered)\n",
    "\n",
    "keywords = freq.most_common(5)\n",
    "print(f\"Keywords: {keywords}\")\n",
    "\n"
   ],
   "id": "bebd436c114d6db1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: [('divided', 2), ('families', 2), ('One', 1), ('consequence', 1), ('even', 1)]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. sent_tokenize\n",
    "\n",
    "sent_tokenize用于将文本分割成句子列表。句子分割的目的是将一段连续的文本分解为单独的句子，以便于后续的分析和处理。"
   ],
   "id": "5e34b7d60fc34126"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:28.345137Z",
     "start_time": "2024-06-14T11:17:28.343203Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import sent_tokenize",
   "id": "dee4404a5d0e1db0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "ca49b5a27c15cfc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:29.915576Z",
     "start_time": "2024-06-14T11:17:29.913191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = [\n",
    "    {\n",
    "        \"text\": \"The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (DPRK) and find ways to revive dialogue with the Government, the UN Security Council heard on Wednesday. \"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Born to a leading family in the capital, Pyongyang, Mr. Kim was 19 when he left to study in Beijing in 2010. Using the internet, he said he learned about his homeland and “the horrific truth” previously hidden to him.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"He too welcomed the OECD figures announced on Wednesday and said there is now an opportunity to consider what the transition to renewable energy really means for SIDS. It amounts to economic transformation, he said.\"\n",
    "    }\n",
    "]\n"
   ],
   "id": "432cdf82468369a8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:17:31.122505Z",
     "start_time": "2024-06-14T11:17:31.119522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = sent_tokenize(text)\n",
    "    print(result)"
   ],
   "id": "4165e2144a63e76e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (DPRK) and find ways to revive dialogue with the Government, the UN Security Council heard on Wednesday.']\n",
      "['Born to a leading family in the capital, Pyongyang, Mr. Kim was 19 when he left to study in Beijing in 2010.', 'Using the internet, he said he learned about his homeland and “the horrific truth” previously hidden to him.']\n",
      "['He too welcomed the OECD figures announced on Wednesday and said there is now an opportunity to consider what the transition to renewable energy really means for SIDS.', 'It amounts to economic transformation, he said.']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:18:10.698008Z",
     "start_time": "2024-06-16T07:18:09.452109Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('averaged_perceptron_tagger')",
   "id": "7f2d5451202a4313",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Honjoutx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:20:04.495737Z",
     "start_time": "2024-06-16T07:20:04.432740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = sent_tokenize(text)\n",
    "    tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in result]\n",
    "    for i, tagged_sentence in enumerate(tagged_sentences, start=1):\n",
    "        print(f\"Sentence {i}: {tagged_sentence}\\n\")\n"
   ],
   "id": "14b9c38d7c87545",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: [('The', 'DT'), ('international', 'JJ'), ('community', 'NN'), ('must', 'MD'), ('continue', 'VB'), ('to', 'TO'), ('pay', 'VB'), ('close', 'JJ'), ('attention', 'NN'), ('to', 'TO'), ('the', 'DT'), ('troubling', 'VBG'), ('human', 'JJ'), ('rights', 'NNS'), ('situation', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Democratic', 'JJ'), ('People', 'NNP'), ('’', 'NNP'), ('s', 'VBZ'), ('Republic', 'NNP'), ('of', 'IN'), ('Korea', 'NNP'), ('(', '('), ('DPRK', 'NNP'), (')', ')'), ('and', 'CC'), ('find', 'VBP'), ('ways', 'NNS'), ('to', 'TO'), ('revive', 'VB'), ('dialogue', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Government', 'NNP'), (',', ','), ('the', 'DT'), ('UN', 'NNP'), ('Security', 'NNP'), ('Council', 'NNP'), ('heard', 'NN'), ('on', 'IN'), ('Wednesday', 'NNP'), ('.', '.')]\n",
      "\n",
      "Sentence 1: [('China', 'NNP'), ('and', 'CC'), ('Russia', 'NNP'), ('opposed', 'VBD'), ('the', 'DT'), ('meeting', 'NN'), ('and', 'CC'), ('called', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('procedural', 'JJ'), ('vote', 'NN'), ('by', 'IN'), ('the', 'DT'), ('15', 'CD'), ('members', 'NNS'), (',', ','), ('which', 'WDT'), ('was', 'VBD'), ('defeated', 'VBN'), ('.', '.')]\n",
      "\n",
      "Sentence 1: [('One', 'CD'), ('consequence', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('divided', 'JJ'), ('families', 'NNS'), ('are', 'VBP'), ('even', 'RB'), ('more', 'RBR'), ('divided', 'JJ'), ('.', '.')]\n",
      "\n",
      "Sentence 2: [('No', 'DT'), ('departures', 'NNS'), ('means', 'VBZ'), ('no', 'DT'), ('reunification', 'NN'), ('with', 'IN'), ('families', 'NNS'), ('abroad', 'RB'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:28:44.153097Z",
     "start_time": "2024-06-16T07:28:44.103454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "sentences = []\n",
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    result = sent_tokenize(text)\n",
    "    for sentence in result :\n",
    "        words = sentence.split()\n",
    "        corrected_words = [spell(word) for word in words]\n",
    "        corrected_sentence = ' '.join(corrected_words)\n",
    "        sentences.append(corrected_sentence)\n",
    "        corrected_text = ' '.join(sentences)\n",
    "        print(f\"corrected: {corrected_text}\")\n",
    "\n"
   ],
   "id": "2dc5d7c3f6f8321d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrected: The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (PRK) and find ways to revive dialogue with the Government, the Up Security Council heard on Wednesday.\n",
      "corrected: The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (PRK) and find ways to revive dialogue with the Government, the Up Security Council heard on Wednesday. China and Russia opposed the meeting and called for a procedural vote by the 15 members, which was defeated.\n",
      "corrected: The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (PRK) and find ways to revive dialogue with the Government, the Up Security Council heard on Wednesday. China and Russia opposed the meeting and called for a procedural vote by the 15 members, which was defeated. One consequence is that divided families are even more divided.\n",
      "corrected: The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (PRK) and find ways to revive dialogue with the Government, the Up Security Council heard on Wednesday. China and Russia opposed the meeting and called for a procedural vote by the 15 members, which was defeated. One consequence is that divided families are even more divided. No departures means no reunification with families abroad.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. RegexpTokenizer\n",
    "\n",
    "RegexpTokenizer用于基于正则表达式进行分词。与word_tokenize和sent_tokenize等函数不同，RegexpTokenizer允许用户使用自定义的正则表达式来定义分词规则，从而实现更灵活和精确的分词。"
   ],
   "id": "4ebcf20f5f7ced16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:36:21.352535Z",
     "start_time": "2024-06-16T07:36:21.349354Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import RegexpTokenizer",
   "id": "eeba9374f30a7c7d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "ea9006ecc8202c6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:23:32.725939Z",
     "start_time": "2024-06-14T11:23:32.722684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = [\n",
    "    {\n",
    "        \"text\": \"The international community must continue to pay close attention to the troubling human rights situation in the Democratic People’s Republic of Korea (DPRK) and find ways to revive dialogue with the Government, the UN Security Council heard on Wednesday. \"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Born to a leading family in the capital, Pyongyang, Mr. Kim was 19 when he left to study in Beijing in 2010. Using the internet, he said he learned about his homeland and “the horrific truth” previously hidden to him.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"He too welcomed the OECD figures announced on Wednesday and said there is now an opportunity to consider what the transition to renewable energy really means for SIDS. It amounts to economic transformation, he said.\"\n",
    "    }\n",
    "]"
   ],
   "id": "6fdd4e4ffd83a4dc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义正则表达式，只保留单词",
   "id": "6639cd0f144aa6cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:33:44.000099Z",
     "start_time": "2024-06-14T12:33:43.997992Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = RegexpTokenizer(r'\\w+')",
   "id": "10a58cf42402cb6c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:33:44.809091Z",
     "start_time": "2024-06-14T12:33:44.806438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(tokens)"
   ],
   "id": "e1daf2c219b45da4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'international', 'community', 'must', 'continue', 'to', 'pay', 'close', 'attention', 'to', 'the', 'troubling', 'human', 'rights', 'situation', 'in', 'the', 'Democratic', 'People', 's', 'Republic', 'of', 'Korea', 'DPRK', 'and', 'find', 'ways', 'to', 'revive', 'dialogue', 'with', 'the', 'Government', 'the', 'UN', 'Security', 'Council', 'heard', 'on', 'Wednesday']\n",
      "['Born', 'to', 'a', 'leading', 'family', 'in', 'the', 'capital', 'Pyongyang', 'Mr', 'Kim', 'was', '19', 'when', 'he', 'left', 'to', 'study', 'in', 'Beijing', 'in', '2010', 'Using', 'the', 'internet', 'he', 'said', 'he', 'learned', 'about', 'his', 'homeland', 'and', 'the', 'horrific', 'truth', 'previously', 'hidden', 'to', 'him']\n",
      "['He', 'too', 'welcomed', 'the', 'OECD', 'figures', 'announced', 'on', 'Wednesday', 'and', 'said', 'there', 'is', 'now', 'an', 'opportunity', 'to', 'consider', 'what', 'the', 'transition', 'to', 'renewable', 'energy', 'really', 'means', 'for', 'SIDS', 'It', 'amounts', 'to', 'economic', 'transformation', 'he', 'said']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义正则表达式，保留单词和标点符号",
   "id": "bab333c847385aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:33:46.336677Z",
     "start_time": "2024-06-14T12:33:46.334172Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')",
   "id": "61861423630f9854",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:33:47.028590Z",
     "start_time": "2024-06-14T12:33:47.025446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for case in cases:\n",
    "    text = case[\"text\"]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(tokens)"
   ],
   "id": "b9788eb2bde6cb21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'international', 'community', 'must', 'continue', 'to', 'pay', 'close', 'attention', 'to', 'the', 'troubling', 'human', 'rights', 'situation', 'in', 'the', 'Democratic', 'People', '’', 's', 'Republic', 'of', 'Korea', '(', 'DPRK', ')', 'and', 'find', 'ways', 'to', 'revive', 'dialogue', 'with', 'the', 'Government', ',', 'the', 'UN', 'Security', 'Council', 'heard', 'on', 'Wednesday', '.']\n",
      "['Born', 'to', 'a', 'leading', 'family', 'in', 'the', 'capital', ',', 'Pyongyang', ',', 'Mr', '.', 'Kim', 'was', '19', 'when', 'he', 'left', 'to', 'study', 'in', 'Beijing', 'in', '2010', '.', 'Using', 'the', 'internet', ',', 'he', 'said', 'he', 'learned', 'about', 'his', 'homeland', 'and', '“', 'the', 'horrific', 'truth', '”', 'previously', 'hidden', 'to', 'him', '.']\n",
      "['He', 'too', 'welcomed', 'the', 'OECD', 'figures', 'announced', 'on', 'Wednesday', 'and', 'said', 'there', 'is', 'now', 'an', 'opportunity', 'to', 'consider', 'what', 'the', 'transition', 'to', 'renewable', 'energy', 'really', 'means', 'for', 'SIDS', '.', 'It', 'amounts', 'to', 'economic', 'transformation', ',', 'he', 'said', '.']\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:36:45.546047Z",
     "start_time": "2024-06-16T07:36:45.543047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = \"Contact us at support@example.com, sales@example.com, or info@example.com.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "tokens = tokenizer.tokenize(cases)\n",
    "\n",
    "print(tokens)\n"
   ],
   "id": "b654a103a9c190ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@example.com', 'sales@example.com', 'info@example.com']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. TreebankWordTokenizer\n",
    "\n",
    "\n",
    "TreebankWordTokenizer是一种基于宾州树库中使用的分词标准而设计的用于自然语言处理的分词器，用于将英语文本分割成单词和标点符号。这种分词器在处理标点符号、缩略词、连字符等方面具有特定的规则。"
   ],
   "id": "70daf776939b1409"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:43:53.572492Z",
     "start_time": "2024-06-16T07:43:53.570298Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import TreebankWordTokenizer",
   "id": "dab87f2183423625",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "bd22ebc4041d7a66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:43:54.540359Z",
     "start_time": "2024-06-16T07:43:54.537854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = [\n",
    "    {\n",
    "        \"text\": \"Born to a leading family in the capital, Pyongyang, Mr. Kim was 19 when he left to study in Beijing in 2010. Using the internet, he said he learned about his homeland and “the horrific truth” previously hidden to him.\"\n",
    "    },\n",
    "]"
   ],
   "id": "ed3119c1b322933d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "文本预处理",
   "id": "51999e5518d70995"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:43:56.341860Z",
     "start_time": "2024-06-16T07:43:56.339415Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = TreebankWordTokenizer()",
   "id": "c265f4f69206cebb",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:43:57.311163Z",
     "start_time": "2024-06-16T07:43:57.308166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = cases[0][\"text\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ],
   "id": "a4fac027b1bafae2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Born', 'to', 'a', 'leading', 'family', 'in', 'the', 'capital', ',', 'Pyongyang', ',', 'Mr.', 'Kim', 'was', '19', 'when', 'he', 'left', 'to', 'study', 'in', 'Beijing', 'in', '2010.', 'Using', 'the', 'internet', ',', 'he', 'said', 'he', 'learned', 'about', 'his', 'homeland', 'and', '“the', 'horrific', 'truth”', 'previously', 'hidden', 'to', 'him', '.']\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "情感分析\n",
    "\n",
    "通过分词提高识别准确性\n",
    "\n",
    "需要额外使用到nltk.sentiment.vader库"
   ],
   "id": "e874095ac82c670a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:41:30.369655Z",
     "start_time": "2024-06-16T07:41:29.483558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ],
   "id": "bf200b931d19f1ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Honjoutx\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:43:59.825589Z",
     "start_time": "2024-06-16T07:43:59.818765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "target_text = \" \".join(tokens)\n",
    "sentiment = analyzer.polarity_scores(target_text)\n",
    "print(sentiment)"
   ],
   "id": "300a7419eab8ed45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.104, 'neu': 0.896, 'pos': 0.0, 'compound': -0.6597}\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "机器翻译\n",
    "\n",
    "作为预处理步骤包括将源语言文本分割成单词，然后进行翻译"
   ],
   "id": "8f10718c095b2f04"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "from googletrans import Translator",
   "id": "40f9462fbb31ee99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-16T07:45:08.508905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "translator = Translator()\n",
    "text = cases[0][\"text\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "translated_tokens = [translator.translate(word, src='en', dest='es').text for word in tokens]\n",
    "translate = ' '.join(translated_tokens)\n",
    "print(f\"Translated Text: {translate}\")"
   ],
   "id": "5b1b014da528a086",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. TweetTokenizer\n",
    "\n",
    "TweetTokenizer是一个专门用于处理推文的分词器。它设计用来处理推文中特有的文本格式，比如表情符号、哈希标签、用户提及、URL等。"
   ],
   "id": "1e458e39069fe839"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:11:35.018960Z",
     "start_time": "2024-06-16T06:11:35.016532Z"
    }
   },
   "cell_type": "code",
   "source": "from nltk.tokenize import TweetTokenizer",
   "id": "53221482bf9096bc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "590bc1ecf5e9881a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:12:41.628122Z",
     "start_time": "2024-06-16T06:12:41.626049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cases = [\n",
    "    \"Thanks for the support, @user1! #grateful\",\n",
    "    \"Had a great time with @user2 and @user3 yesterday! #friends\",\n",
    "    \"Shoutout to @user4 for the amazing work! #appreciation\"\n",
    "]"
   ],
   "id": "68ea25f3c246c1d6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "话题提取",
   "id": "1776cb692d1a3bb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:11:37.344597Z",
     "start_time": "2024-06-16T06:11:37.341585Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = TweetTokenizer()",
   "id": "c143db33afa803e7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:12:43.508365Z",
     "start_time": "2024-06-16T06:12:43.504886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tags = []\n",
    "for tweet in cases:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    tags.extend([token for token in tokens if token.startswith('#')])\n",
    "\n",
    "print(\"tags:\", tags)"
   ],
   "id": "2cccd127e171a8f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags: ['#grateful', '#friends', '#appreciation']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "用户提及分析",
   "id": "2a73c51ebef1716b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:13:36.156982Z",
     "start_time": "2024-06-16T06:13:36.154683Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = TweetTokenizer()",
   "id": "2bd519db5741cb42",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T06:13:37.103129Z",
     "start_time": "2024-06-16T06:13:37.099720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "users = []\n",
    "for tweet in cases:\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    users.extend([token for token in tokens if token.startswith('@')])\n",
    "    \n",
    "print(\"User Mentions:\", users)"
   ],
   "id": "fe288d453338dbcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Mentions: ['@user1', '@user2', '@user3', '@user4']\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
