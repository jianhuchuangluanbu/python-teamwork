使用NLTK中的nltk.tokenize模块。
nltk.tokenize模块专门用于分词，即将文本拆分成单词、句子或其他标记的过程。

**01** word_tokenize
    word_tokenize函数可以将字符串分割成单词列表，分词的目的是将一段连续的文本分解为更小的单位，以便于后续的分析和处理。
    测试样例包括连续文本分词，词频统计，提取常见关键词。

**02** sent_tokenize
    sent_tokenize用于将文本分割成句子列表。句子分割的目的是将一段连续的文本分解为单独的句子，以便于后续的分析和处理。
    测试样例包括连续文本分句，词性标注，拼写纠正。

**03** RegexpTokenizer
    RegexpTokenizer用于基于正则表达式进行分词。
    与word_tokenize和sent_tokenize等函数不同，RegexpTokenizer允许用户使用自定义的正则表达式来定义分词规则，从而实现更灵活和精确的分词。
    测试样例包括按正则分词，按正则分词和标点，按正则分邮件地址。

**04** TreebankWordTokenizer
    TreebankWordTokenizer是一种基于宾州树库中使用的分词标准而设计的用于自然语言处理的分词器，用于将英语文本分割成单词和标点符号。
    这种分词器在处理标点符号、缩略词、连字符等方面具有特定的规则。
    测试样例包括文本预处理（分词、标点、数字、缩略词），分析情感倾向，拆词翻译再连接。

**05** TweetTokenizer
    TweetTokenizer是一个专门用于处理推文的分词器。它设计用来处理推文中特有的文本格式，比如表情符号、哈希标签、用户提及、URL等。
    测试样例包括话题提取（推文分词，提取标签），用户提及分析。
