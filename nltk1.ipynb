{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "介绍NLTK 中的 nltk.corpus 模块\n",
    "\n",
    "nltk.corpus 模块主要用于访问和处理各种语料库（corpora）。语料库是文本和语言数据的集合，用于语言学研究、开发和评估自然语言处理（NLP）系统。nltk.corpus 模块提供了许多常见语言资源的接口，使得开发者可以方便地获取和使用这些资源。\n"
   ],
   "id": "598e00ca69238196"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. stopwords\n",
    "\n",
    "stopwords 方法提供了常见的停用词列表，可以帮助在文本预处理阶段去除无意义的词语。"
   ],
   "id": "db56ae0070961b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:16:34.118674Z",
     "start_time": "2024-06-16T15:16:34.113841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.data.path.append(r'.\\nltk_data')\n",
    "nltk.download('stopwords')"
   ],
   "id": "86c33b80ce0e6681",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:03.497660Z",
     "start_time": "2024-06-16T15:17:03.493584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 加载英语停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 打印停用词列表\n",
    "print(stop_words)"
   ],
   "id": "bb40198cc63760b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'when', 'at', 'into', 'in', 'off', 'its', 'ain', 'don', 'only', 'me', 'themselves', \"it's\", 'once', 'yourselves', 'have', 'each', \"couldn't\", 'were', 'y', 'to', \"wouldn't\", 'through', \"didn't\", \"you'll\", 'mustn', 'nor', 'having', 'until', 'no', 'below', \"you're\", 'than', 'has', 'an', \"mightn't\", 'them', 'as', 'most', 'other', \"hadn't\", 'he', 'aren', 'too', 'it', \"haven't\", 'didn', \"mustn't\", 's', 'o', \"you'd\", 'ours', 'theirs', 'd', 'that', 'out', 'down', 'shan', 'now', 'being', \"don't\", 'him', 'couldn', \"isn't\", \"hasn't\", 'shouldn', 'been', 'weren', 't', 'why', 'those', 'their', 'a', 'where', 'herself', 'because', 'but', 'did', 'our', 'here', 'hasn', 'while', 'or', 'some', 'if', 'from', 'doesn', \"needn't\", 'yourself', 'your', 'm', 'she', \"weren't\", 'himself', 'wasn', 'am', 'after', 'all', \"shouldn't\", 'hadn', 'will', 'such', 'before', 'his', 'does', 'won', 'these', 'same', 'just', 'for', \"shan't\", 'about', 'of', 'there', 'doing', 'over', 'which', 'what', 'then', 'so', 'above', 'can', \"aren't\", 'mightn', \"doesn't\", 'isn', 'ma', 've', 'is', 'be', 'and', \"you've\", 'they', 're', \"she's\", 'wouldn', 'during', \"wasn't\", 'again', 'haven', 'ourselves', \"won't\", 'up', 'on', 'both', 'i', 'yours', 'hers', 'very', 'myself', 'her', 'do', 'whom', 'with', 'should', 'who', \"should've\", 'needn', 'few', 'was', 'my', \"that'll\", 'any', 'own', 'we', 'how', 'against', 'by', 'not', 'the', 'had', 'between', 'under', 'further', 'you', 'more', 'this', 'll', 'are'}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "bf9c4f283704f038"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "基本文本处理：使用停用词列表去除文本中的停用词，验证文本处理后的结果是否符合预期。\n",
    "\n",
    "先创建一个text，分词后将words 列表中过滤掉存在于 stop_words 集合中的停用词，并保存到filtered_words，与预期结果对比不对就报错"
   ],
   "id": "2fa1b39929c94198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:09.910489Z",
     "start_time": "2024-06-16T15:17:09.906093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def processing():\n",
    "    text = \"This is an example sentence to demonstrate stopwords removal.\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 分词\n",
    "    words = text.split()  \n",
    "    \n",
    "    # 去除停用词\n",
    "\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # 预期结果\n",
    "    expected_output = \"example sentence demonstrate stopwords removal.\"\n",
    "    \n",
    "    # 将列表转换为字符串\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    \n",
    "    # 预期输出与处理后的文本是否相等\n",
    "    assert processed_text == expected_output, f\"Expected: {expected_output}, but got: {processed_text}\"\n",
    "\n",
    "processing()\n"
   ],
   "id": "21759f5303369c6f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "处理空文本：测试在处理空文本时是否能够正常工作，即不引发错误并返回预期的空字符串。\n",
   "id": "6680c68b018aa042"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:12.708909Z",
     "start_time": "2024-06-16T15:17:12.704177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def empty_text():\n",
    "    text = \"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 分词\n",
    "    words = text.split()\n",
    "    \n",
    "    # 去除停用词\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # 预期结果是空字符串\n",
    "    expected_output = \"\"\n",
    "    \n",
    "    # 将列表转换为字符串\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    \n",
    "    # 输出与处理后的文本是否相等\n",
    "    assert processed_text == expected_output, f\"Expected: {expected_output}, but got: {processed_text}\"\n",
    "\n",
    "empty_text()\n"
   ],
   "id": "cdc4f0fa48f89820",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "处理全是停用词的文本：测试当输入文本全部由停用词组成时的处理结果，预期结果应该是空字符串。",
   "id": "278bdddd4fe2890e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:15.197858Z",
     "start_time": "2024-06-16T15:17:15.192388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def all_stopwords():\n",
    "    text = \"the and a of to\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 分词\n",
    "    words = text.split()\n",
    "    \n",
    "    # 去除停用词\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # 预期结果是空字符串\n",
    "    expected_output = \"\"\n",
    "    \n",
    "    # 将列表转换为字符串\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    \n",
    "    # 预期输出与处理后的文本是否相等\n",
    "    assert processed_text == expected_output, f\"Expected: {expected_output}, but got: {processed_text}\"\n",
    "\n",
    "all_stopwords()\n"
   ],
   "id": "7caad82e5fb0fd0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a86d8190924b2072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. wordnet\n",
    "\n",
    "wordnet 是一个英语词汇数据库，包含了大量的同义词集合（synsets）和词语之间的语义关系。"
   ],
   "id": "fcec5a780d7a2032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:26.144990Z",
     "start_time": "2024-06-16T15:17:22.147352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "7d13d32b13015b31",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:30.493515Z",
     "start_time": "2024-06-16T15:17:29.546028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 查找单词的同义词集合\n",
    "synsets = wordnet.synsets('car')\n",
    "for synset in synsets:\n",
    "    print(synset.name(), ':', synset.definition())"
   ],
   "id": "7bf6d750ce50bdb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car.n.01 : a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "car.n.02 : a wheeled vehicle adapted to the rails of railroad\n",
      "car.n.03 : the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "car.n.04 : where passengers ride up and down\n",
      "cable_car.n.01 : a conveyance for passengers or freight on a cable railway\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "d2ae5ed8202d5924"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "基本的同义词查找：测试基本的同义词查找功能，确保能正确地找到指定单词的同义词集合。",
   "id": "72553172f0c493b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:35.519363Z",
     "start_time": "2024-06-16T15:17:35.515665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(word):\n",
    "    # 查找单词的同义词集合\n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    # 预期至少会找到一个同义词集合\n",
    "    assert len(synsets) > 0, f\"Expected to find synonyms for '{word}', but found none.\"\n",
    "    \n",
    "    # 输出找到的同义词\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    \n",
    "    return synonyms\n",
    "\n",
    "word = 'car'\n",
    "synonyms = search(word)\n",
    "print(f\"Synonyms for '{word}': {synonyms}\")"
   ],
   "id": "4b6ef887c2b32c02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for 'car': {'gondola', 'cable_car', 'machine', 'railway_car', 'auto', 'car', 'automobile', 'elevator_car', 'railroad_car', 'railcar', 'motorcar'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "同义词集合名称的格式：测试同义词集合名称的格式是否符合预期。\n",
    "测试用例验证同义词集合名称是否遵循形如 'word.a.n' 的标准格式，例如 'car.n.01' 等。"
   ],
   "id": "85d7a97cc3c3da50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:38.009791Z",
     "start_time": "2024-06-16T15:17:38.004974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def definitions():\n",
    "    word = 'car'\n",
    "    \n",
    "    # 查找单词的同义词集合\n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    # 检查每个同义词的定义是否非空\n",
    "    for synset in synsets:\n",
    "        assert synset.definition(), f\"Definition for synonym '{synset.name()}' is empty or None.\"\n",
    "\n",
    "definitions()"
   ],
   "id": "97c89ea697a3f872",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. treebank\n",
    "\n",
    "treebank 是一个包含了已标注的英文句子语料库，用于训练和评估词性标注器和其他句法分析工具。"
   ],
   "id": "f50f785a285004b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:42.206116Z",
     "start_time": "2024-06-16T15:17:41.116137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ],
   "id": "4e933498de2fef90",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:44.443244Z",
     "start_time": "2024-06-16T15:17:44.413347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# 获取句子\n",
    "sentences = treebank.sents()\n",
    "print(sentences[0])"
   ],
   "id": "76484c6d890aa941",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "9034cb7024db10cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "基本的句子获取：测试基本的句子获取功能，确保能正确地从 treebank 中获取句子。",
   "id": "237ca96879a44eee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:48.758398Z",
     "start_time": "2024-06-16T15:17:48.229530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sentence():\n",
    "    # 获取句子\n",
    "    sentences = treebank.sents()\n",
    "    \n",
    "    # 预期至少会找到一个句子\n",
    "    assert len(sentences) > 0, \"Expected to find sentences in treebank, but found none.\"\n",
    "    \n",
    "    # 输出找到的句子\n",
    "    print(\"Sample sentences from treebank:\")\n",
    "    for index, sentence in enumerate(sentences[:5]):  # 输出前五个句子作为示例\n",
    "        print(f\"{index + 1}. {' '.join(sentence)}\")\n",
    "\n",
    "# 输出\n",
    "sentence()"
   ],
   "id": "b11b168a6b1d88c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentences from treebank:\n",
      "1. Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "2. Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .\n",
      "3. Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate .\n",
      "4. A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 .\n",
      "5. The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 .\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "句子格式检查：测试获取的句子是否是符合预期的格式，例如列表中的每个元素应当是一个由单词组成的列表。",
   "id": "ced4dec12b3626bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:50.723019Z",
     "start_time": "2024-06-16T15:17:50.194693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format():\n",
    "    # 获取句子\n",
    "    sentences = treebank.sents()\n",
    "    \n",
    "    # 检查每个句子的格式\n",
    "    for sentence in sentences:\n",
    "        assert isinstance(sentence, list) and all(isinstance(word, str) for word in sentence), \\\n",
    "            f\"Invalid format for sentence: {sentence}\"\n",
    "\n",
    "format()"
   ],
   "id": "19694da4467ce18e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "句子数量检查：测试获取的句子数量是否符合预期，例如至少应当能获取到指定数量的句子。",
   "id": "5501d3dccb81196a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:53.796210Z",
     "start_time": "2024-06-16T15:17:53.281459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count():\n",
    "    expected_sentence_count = 500  # 假设预期至少能获取到500个句子\n",
    "    \n",
    "    # 获取句子\n",
    "    sentences = treebank.sents()\n",
    "    \n",
    "    # 检查获取的句子数量是否符合预期\n",
    "    assert len(sentences) >= expected_sentence_count, \\\n",
    "        f\"Expected at least {expected_sentence_count} sentences, but found {len(sentences)}.\"\n",
    "\n",
    "count()"
   ],
   "id": "e207a88706fd4169",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. brown\n",
    "\n",
    "brown 是一个经典的英语语料库，包含了不同类型的文本（新闻、社论、小说等）。"
   ],
   "id": "3de435131df13a93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:58.746696Z",
     "start_time": "2024-06-16T15:17:56.976822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ],
   "id": "486f7223d0134b65",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:17:59.951214Z",
     "start_time": "2024-06-16T15:17:59.879648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 下载并加载 'brown' 语料库（如果还没有下载的话）\n",
    "nltk.download('brown')\n",
    "\n",
    "# 获取文本类型和对应的文本\n",
    "genres = brown.categories()\n",
    "print(\"Available genres:\", genres)\n",
    "\n",
    "# 选择一个文本类型\n",
    "selected_genre = 'news'\n",
    "\n",
    "# 获取选定文本类型的句子列表\n",
    "sentences = brown.sents(categories=selected_genre)\n",
    "\n",
    "# 打印前5个句子\n",
    "print(f\"\\nSample sentences from '{selected_genre}' genre:\")\n",
    "for index, sentence in enumerate(sentences[:5], 1):\n",
    "    print(f\"Sentence {index}: {' '.join(sentence)}\")\n"
   ],
   "id": "50942caa7e5735fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available genres: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n",
      "Sample sentences from 'news' genre:\n",
      "Sentence 1: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "Sentence 2: The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
      "Sentence 3: The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\n",
      "Sentence 4: `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
      "Sentence 5: The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "1825a9b817694b8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "基本的文本类型获取：测试基本的文本类型获取功能，确保能正确地从 brown 包中获取文本类型。",
   "id": "86e9119a66678051"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:31.697609Z",
     "start_time": "2024-06-16T15:18:31.694190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieval():\n",
    "    # 获取文本类型\n",
    "    genres = brown.categories()\n",
    "    \n",
    "    # 预期至少会找到一个文本类型\n",
    "    assert len(genres) > 0, \"Expected to find genres in brown corpus, but found none.\"\n",
    "    \n",
    "    # 输出找到的文本类型\n",
    "    print(\"Genres found in Brown corpus:\")\n",
    "    for genre in genres:\n",
    "        print(genre)\n",
    "\n",
    "# 输出\n",
    "retrieval()"
   ],
   "id": "739424ced3cc9ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres found in Brown corpus:\n",
      "adventure\n",
      "belles_lettres\n",
      "editorial\n",
      "fiction\n",
      "government\n",
      "hobbies\n",
      "humor\n",
      "learned\n",
      "lore\n",
      "mystery\n",
      "news\n",
      "religion\n",
      "reviews\n",
      "romance\n",
      "science_fiction\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "句子格式检查：测试获取的句子是否是符合预期的格式，例如列表中的每个元素应当是一个由单词组成的列表。\n",
   "id": "b871fdf5fa54a3ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:34.718012Z",
     "start_time": "2024-06-16T15:18:34.709817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format():\n",
    "    # 选择一个文本类型\n",
    "    selected_genre = 'news'\n",
    "    \n",
    "    # 获取选定文本类型的句子列表\n",
    "    sentences = brown.sents(categories=selected_genre)\n",
    "    \n",
    "    # 检查每个句子的格式\n",
    "    for sentence in sentences[:5]:\n",
    "        assert isinstance(sentence, list) and all(isinstance(word, str) for word in sentence), \\\n",
    "            f\"Invalid format for sentence: {sentence}\"\n",
    "\n",
    "format()"
   ],
   "id": "bfb3e59eca99a330",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "文本类型数量检查：测试获取的文本类型数量是否符合预期，例如至少应当能获取到指定数量的文本类型。",
   "id": "513fe7a4f77908e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:36.165887Z",
     "start_time": "2024-06-16T15:18:35.975127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count():\n",
    "    # 选择一个文本类型\n",
    "    selected_genre = 'news'\n",
    "    \n",
    "    # 获取选定文本类型的句子列表\n",
    "    sentences = brown.sents(categories=selected_genre)\n",
    "    \n",
    "    expected_sentence_count = 500  # 假设预期至少能获取到500个句子\n",
    "    \n",
    "    # 检查获取的句子数量是否符合预期\n",
    "    assert len(sentences) >= expected_sentence_count, \\\n",
    "        f\"Expected at least {expected_sentence_count} sentences, but found {len(sentences)}.\"\n",
    "\n",
    "\n",
    "count()"
   ],
   "id": "2746a4b23aa2ea64",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. gutenberg\n",
    "\n",
    "gutenberg 是一个包含了古腾堡计划中的文本的语料库，包括了多种公共领域的文学作品。"
   ],
   "id": "fe4adf0a813d3cf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:38.444008Z",
     "start_time": "2024-06-16T15:18:37.034149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n"
   ],
   "id": "30d3d19576df2138",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\31542\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:38.477268Z",
     "start_time": "2024-06-16T15:18:38.445011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# 获取文本\n",
    "text = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "print(text[:500])\n"
   ],
   "id": "4f00779dbc61d8a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Hamlet by William Shakespeare 1599]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Enter Barnardo and Francisco two Centinels.\n",
      "\n",
      "  Barnardo. Who's there?\n",
      "  Fran. Nay answer me: Stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   Bar. Long liue the King\n",
      "\n",
      "   Fran. Barnardo?\n",
      "  Bar. He\n",
      "\n",
      "   Fran. You come most carefully vpon your houre\n",
      "\n",
      "   Bar. 'Tis now strook twelue, get thee to bed Francisco\n",
      "\n",
      "   Fran. For this releefe much thankes: 'Tis bitter cold,\n",
      "And I am sicke at heart\n",
      "\n",
      "   Barn. Haue you had quiet Guard?\n",
      "  Fran. Not\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "基本的文本获取：测试基本的文本获取功能，确保能正确地从 gutenberg 包中获取指定文件的文本数据。",
   "id": "2aafbaec51a7abe9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:39.086512Z",
     "start_time": "2024-06-16T15:18:39.083029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieval():\n",
    "    # 获取文本\n",
    "    text = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "    \n",
    "    # 预期文本长度大于0\n",
    "    assert len(text) > 0, \"Expected to retrieve text from gutenberg corpus, but found empty.\"\n",
    "\n",
    "retrieval()"
   ],
   "id": "bbe3f7610acf63c0",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "文本内容检查：测试获取的文本内容是否符合预期，例如文本的前几个字符应当是预期的片段。",
   "id": "ff7d2336cac78ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:40.634717Z",
     "start_time": "2024-06-16T15:18:40.076823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def content():\n",
    "    expected_start_text = \"ACT I. SCENE I.\"\n",
    "    \n",
    "    # 获取文本\n",
    "    text = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "    \n",
    "    # 检查文本内容是否包含预期的起始文本\n",
    "    assert expected_start_text in text, \\\n",
    "        f\"Expected text to contain '{expected_start_text}', but it was not found in the text.\"\n",
    "\n",
    "content()"
   ],
   "id": "247e80919dd8a6ad",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected text to contain 'ACT I. SCENE I.', but it was not found in the text.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 11\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# 检查文本内容是否包含预期的起始文本\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m expected_start_text \u001B[38;5;129;01min\u001B[39;00m text, \\\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected text to contain \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_start_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, but it was not found in the text.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 11\u001B[0m content()\n",
      "Cell \u001B[1;32mIn[32], line 8\u001B[0m, in \u001B[0;36mcontent\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m text \u001B[38;5;241m=\u001B[39m gutenberg\u001B[38;5;241m.\u001B[39mraw(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshakespeare-hamlet.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 检查文本内容是否包含预期的起始文本\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m expected_start_text \u001B[38;5;129;01min\u001B[39;00m text, \\\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected text to contain \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_start_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, but it was not found in the text.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Expected text to contain 'ACT I. SCENE I.', but it was not found in the text."
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "文本长度检查：测试获取的文本长度是否符合预期，例如文本长度应当大于或等于指定的最小长度。",
   "id": "4806146f1bb25a91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T15:18:41.059924Z",
     "start_time": "2024-06-16T15:18:41.055019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def length():\n",
    "    min_expected_length = 1000  # 假设预期文本长度至少为1000个字符\n",
    "    \n",
    "    # 获取文本\n",
    "    text = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "    \n",
    "    # 检查\n",
    "    assert len(text) >= min_expected_length, \\\n",
    "        f\"Expected text length to be at least {min_expected_length}, but found {len(text)}.\"\n",
    "\n",
    "length()"
   ],
   "id": "d2d0bc8afbc02da8",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d0c1bc49528ae0fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
